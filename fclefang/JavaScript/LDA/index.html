<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>PCA and LDA | fclefang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="&amp;emsp;&amp;emsp;今天先对线性判别分析(LDA)做一个总结。LDA应用非常广泛，但也要注意今天理解的LDA和NLP领域的LDA(隐含狄利克雷分布Latent Dirichlet Allocation)是不一样的，它是一种处理文档的主题模型。本文根据网络上其他同学关于LDA的理解，整合成符合自己理解方式的推导思路。">
<meta property="og:type" content="article">
<meta property="og:title" content="PCA and LDA">
<meta property="og:url" content="http://www.fclef.me/fclefang/JavaScript/LDA/index.html">
<meta property="og:site_name" content="fclefang">
<meta property="og:description" content="&amp;emsp;&amp;emsp;今天先对线性判别分析(LDA)做一个总结。LDA应用非常广泛，但也要注意今天理解的LDA和NLP领域的LDA(隐含狄利克雷分布Latent Dirichlet Allocation)是不一样的，它是一种处理文档的主题模型。本文根据网络上其他同学关于LDA的理解，整合成符合自己理解方式的推导思路。">
<meta property="og:image" content="http://ogy25dae4.bkt.clouddn.com/LDA%E6%8A%95%E5%BD%B1.png">
<meta property="og:image" content="http://ogy25dae4.bkt.clouddn.com/LDA_image2.png">
<meta property="og:updated_time" content="2018-07-08T15:37:48.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PCA and LDA">
<meta name="twitter:description" content="&amp;emsp;&amp;emsp;今天先对线性判别分析(LDA)做一个总结。LDA应用非常广泛，但也要注意今天理解的LDA和NLP领域的LDA(隐含狄利克雷分布Latent Dirichlet Allocation)是不一样的，它是一种处理文档的主题模型。本文根据网络上其他同学关于LDA的理解，整合成符合自己理解方式的推导思路。">
<meta name="twitter:image" content="http://ogy25dae4.bkt.clouddn.com/LDA%E6%8A%95%E5%BD%B1.png">
  
    <link rel="alternative" href="/atom.xml" title="fclefang" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/timg.png">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="/img/timg.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Fclefang</a></h1>
		</hgroup>

		
		<p class="header-subtitle">开心自由最重要</p>
		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						
						<li>About</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/categories">分类</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/MR-Fclef" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
					        
								<a class="rss" target="_blank" href="#" title="rss">rss</a>
					        
								<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/fang-xiang-yan" title="zhihu">zhihu</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/GIS/" style="font-size: 12.5px;">GIS</a> <a href="/tags/JavaScript/" style="font-size: 15px;">JavaScript</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/MachineLearning/" style="font-size: 20px;">MachineLearning</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/Node-js/" style="font-size: 12.5px;">Node.js</a> <a href="/tags/gossip/" style="font-size: 10px;">gossip</a> <a href="/tags/java/" style="font-size: 17.5px;">java</a> <a href="/tags/parking/" style="font-size: 10px;">parking</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">CIT（2012-2016）,GuiLin Elec(2016-2019).认真做好应该做的每一件事。</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Fclefang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="/img/timg.jpg" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">Fclefang</h1>
			</hgroup>
			
			<p class="header-subtitle">开心自由最重要</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/categories">分类</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/MR-Fclef" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
			        
						<a class="rss" target="_blank" href="#" title="rss">rss</a>
			        
						<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/fang-xiang-yan" title="zhihu">zhihu</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap"><article id="post-JavaScript/LDA" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/fclefang/JavaScript/LDA/" class="article-date">
  	<time datetime="2018-07-06T01:15:05.000Z" itemprop="datePublished">2018-07-06</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      PCA and LDA
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MachineLearning/">MachineLearning</a></li></ul>
	</div>

        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/LDA/">LDA</a>
	</div>


        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>&emsp;&emsp;今天先对线性判别分析(LDA)做一个总结。LDA应用非常广泛，但也要注意今天理解的LDA和NLP领域的LDA(隐含狄利克雷分布Latent Dirichlet Allocation)是不一样的，它是一种处理文档的主题模型。本文根据网络上其他同学关于LDA的理解，整合成符合自己理解方式的推导思路。</p>
<a id="more"></a>
<hr>
<h2 id="1-LDA的概念"><a href="#1-LDA的概念" class="headerlink" title="1. LDA的概念"></a>1. LDA的概念</h2><p>&emsp;&emsp;LDA是一种监督学习的降维技术，这里和PCA不同的一个关键点在于，同为降维技术，PCA属于无监督学习降维，因为PCA不考虑样本的类别，而LDA是要确定样本类别标签的。LDA的核心思想一句话概括：<strong>投影后类内方差最小，类间方差最大</strong>。也就是说，我们将数据在低维度上投影，投影效果使得每一种类别数据的投影点尽可能接近，而不同类别的数据的类别中心点之间距离尽可能的大。<img src="http://ogy25dae4.bkt.clouddn.com/LDA%E6%8A%95%E5%BD%B1.png" alt="LDA投影"></p>
<p>&emsp;&emsp;举个简单的例子，如上图所示有两类数据，分别为红色和蓝色，可以看大这些红蓝点是二维坐标点，可以理解为每个点有两个特征维度，LDA目的就是找到这样一条一维直线，当所有二维点投影到这样一条直线上，每一种类别在直线上的投影尽可能接近，二不同类别即红蓝中心点的距离尽可能远。从图上就能看到右边的投影方式更好一点。当然实际情况的数据点不一定是二维的，也可能三维、四维等等，那么LDA的目标对应的就是找到更低维度的超平面。</p>
<h2 id="2-LDA原理-以两个类别为例"><a href="#2-LDA原理-以两个类别为例" class="headerlink" title="2. LDA原理(以两个类别为例)"></a>2. LDA原理(以两个类别为例)</h2><p>&emsp;&emsp;这一段内容将以两类数据为例，严谨推导LDA的原理。</p>
<p>&emsp;&emsp;已知数据集${(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$，其中任意样本$x_i$为n维向量，$y_i \in {0,1}$。我们定义$N_j(j=0,1)$为第j类样本的个数，$X_j(j=0,1)$为第j类样本的集合，而$\mu_j(j=0,1)$为第j类样本的均值向量，定义$\Sigma_j(j=0,1)$为第j类样本的协方差矩阵(严格来说是缺少分母部分的协方差矩阵)。$\mu_j$的表达式：<br>$$<br>\mu_j=\frac{1}{N<em>j}\sum</em>{x\in X_j}x<br>$$</p>
<p>&emsp;&emsp;$\Sigma_j$的表达式:<br>$$<br>\Sigma<em>j = \sum</em>{x\in X_j}(x-\mu_j)(x-\mu_j)^T ,(j=0,1)<br>$$<br>&emsp;&emsp;假设我们将这两类数据投影到一条直线上，投影直线是向量$\omega$    ，则对任意一个样本$x_i$，它在直线$\omega$上的投影为$\omega^Tx_i$，对于已知两个类别的中心点$\mu_0,\mu_1$，在直线$\omega$的投影为$\omega^T\mu_0$和$\omega^T\mu_1$。之前说过LDA的思想之一就是让不同类别的类中心之间距离尽可能的大，即最大化$||\omega^T\mu_0-\omega^T\mu_1||^2_2$，另外LDA希望同一类别的数据投影点尽可能接近，也就是同类样本投影点的协方差尽可能的小，即最小化$\omega^T\Sigma_0\omega+\omega^T\Sigma<em>1\omega$。综上可得优化目标:<br>$$<br>arg max</em>{\omega} J(\omega)=\frac{||\omega^T\mu_0-\omega^T\mu_1||^2_2}{\omega^T\Sigma_0\omega+\omega^T\Sigma_1\omega}=\frac{\omega^T(\mu_0-\mu_1)(\mu_0-\mu_1)^T\omega}{\omega^T(\Sigma_0+\Sigma_1)\omega}<br>$$<br>&emsp;&emsp;定义散度矩阵$S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T$，$S_w=(\Sigma_0+\Sigma<em>1)$，优化目标为：<br>$$<br>arg max</em>{\omega} J(\omega)=\frac{\omega^TS_b\omega}{\omega^TS_w\omega}<br>$$<br>&emsp;&emsp;优化目标函数中，$\omega$是非零向量，而分子分母都是关于$\omega$的二次项，因此优化函数的解与$\omega$的长度无关，只与其方向有关。不失一般性，令$\omega^TS_w\omega=1$，即优化函数化简为其分子部分，且受$\omega^TS<em>w\omega=1$的约束。<br>$$<br>arg max</em>{\omega} J(\omega)=\omega^TS_b\omega<br>$$</p>
<p>$$<br>st.\omega^TS_w\omega=1<br>$$</p>
<p>&emsp;&emsp;对于求解含有约束项的优化函数，考虑拉格朗日乘子法，加入拉格朗日乘子并求导可得：<br>$$<br>c(\omega)=\omega^TS_b\omega-\lambda(\omega^TS_w\omega-1)<br>$$</p>
<p>$$<br>\frac{dc}{d\omega}=2S_b\omega-2\lambda S_w\omega=0<br>$$</p>
<p>$$<br>S_b\omega=\lambda S_w\omega<br>$$</p>
<p>&emsp;&emsp;利用矩阵微积分知识，求导过程中可以将$\omega^TS_w\omega$看作$S_w\omega^2$，如果$S_w$可逆(非奇异矩阵，<strong>这里有一点需要注意，小样本问题中即样本的维度小于样本的个数，可以通过PCA先降再利用LDA接着分析</strong>)，那么将求导后的结果两边都乘上$S_w^{-1}$，得到：<br>$$<br>S_w^{-1}S_b\omega=\lambda \omega<br>$$<br>&emsp;&emsp;到了这一步就可以发现其实$\omega$是矩阵$S_w^{-1}S_b$ 的特征向量。这个公式也就是Fisher Linear Discrimination。</p>
<p>&emsp;&emsp;这个时候将$S_b$的公式的代入可得：<br>$$<br>S_b\omega=(\mu_0-\mu_1)(\mu_0-\mu_1)^T\omega=(\mu_0-\mu_1)*\lambda_w<br>$$</p>
<p>$$<br>S_w^{-1}S_b\omega=S_w^{-1}(\mu_0-\mu_1)*\lambda_w=\lambda \omega<br>$$</p>
<p>&emsp;&emsp;之前就说过优化函数的解与$\omega$的大小无关，通俗点说就是对$\omega$扩大缩小任何倍数都不会影响结果，因此我们可以约去上式两边的$\lambda_w$和$\lambda$。最终得到：<br>$$<br>\omega=S_w^{-1}(\mu_0-\mu_1)<br>$$<br>&emsp;&emsp;因此，根据LDA原理,我们只需要求出原始样本的均值和方差就可以求出最佳的方向$\omega$，这其实也就是Fisher在1936年提出的线性判别分析。</p>
<h2 id="3-多类LDA原理分析"><a href="#3-多类LDA原理分析" class="headerlink" title="3. 多类LDA原理分析"></a>3. 多类LDA原理分析</h2><p>&emsp;&emsp;这一段内容将以多类数据为例，严谨推导LDA的原理。</p>
<p>&emsp;&emsp;已知数据集${(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$，其中任意样本$x_i$为n维向量，$y_i \in {C_1,C_2,…,C_k}$。我们定义$N_j(j=1,2,…,k)$为第j类样本的个数，$X_j(j=1,2,…,k)$为第j类样本的集合，而$\mu_j(j=1,2,…,k)$为第j类样本的均值向量，定义$\Sigma_j(j=1,2,…,k)$为第j类样本的协方差矩阵(严格来说是缺少分母部分的协方差矩阵)。</p>
<p>&emsp;&emsp;这里我们将多类样本数据向低纬度投影，而低维空间不再是原本的一条直线，而是一个超平面。假设我们投影的低维空间维度为d，对应的低维空间基向量为$(w_1,w_2…,w_d)$，基向量组成的矩阵为$W$，它是一个$n\times d$的矩阵。</p>
<p>&emsp;&emsp;接下来优化函数的推导过程和二分类情况一致，最终我们可以得到：<br>$$<br>arg max_{W} J(W)=\frac{W^TS_bW}{W^TS_wW}<br>$$</p>
<p>&emsp;&emsp;其中：<br>$$<br>S<em>b=\sum^k</em>{j=1}N_j(\mu_j-\mu)(\mu_j-\mu)^T<br>$$</p>
<p>$$<br>S<em>w = \sum^k</em>{j=1}\sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T<br>$$</p>
<p>&emsp;&emsp;原本在二类LDA分析中$S_b$度量了两个类中心均值点的散列情况，而在多类情况下是度量每类中心均值点相对于所有样本的均值中心的散列情况。但是每一类的样本数量各不相同，所以这里认为样本越多权重越大，权重用$\frac{N_i}{N}$表示，因为优化函数分子分母的倍化并不会影响最终求解，因此直接忽略N这个定常量，直接用$N_i$表示。</p>
<p><img src="http://ogy25dae4.bkt.clouddn.com/LDA_image2.png" alt="LDA_image2"></p>
<p>&emsp;&emsp;我们可以知道矩阵的实际意义是一个协方差矩阵，这个矩阵所刻画的是该类与样本总体之间的关系。矩阵对角线元素是该类相对样本总体的方差（即分散度），非对角线元素是该类样本总体均值的协方差（即该类和总体样本的相关联度或称冗余度），$S_b$即是各个样本根据自己所属的类计算出样本与总体的协方差矩阵的总和，这从宏观上描述了所有类和总体之间的离散冗余程度。同理$S_w$为分类内各个样本和所属类之间的协方差矩阵之和，它所刻画的是从总体来看类内各个样本与所属类之间（这里所刻画的类特性是由是类内各个样本的平均值矩阵构成）离散度，其实从中可以看出不管是类内的样本期望矩阵还是总体样本期望矩阵，它们都只是充当一个媒介作用，不管是类内还是类间离散度矩阵都是从宏观上刻画出类与类之间的样本的离散度和类内样本和样本之间的离散度。</p>
<p>&emsp;&emsp;多分类情况下，由于我们得到的优化函数的饿分子和分母均是散列矩阵不是标量，无法作为一个标量函数来优化求解，因此考虑采取行列式将矩阵变为实数进行求解。行列式的值实际上是矩阵特征值的积，一个特征值可以表示在该特征向量上的发散程度。</p>
<p>&emsp;&emsp;于是，整个优化问题又变成了求解$J(W)$ 最大值，同二分类情况，固定分母为1，然后求偏导并令其等于0求解，得到最终结果：<br>$$<br>S_bw_i = \lambda S_ww_i<br>$$</p>
<p>$$<br>S_w^{-1}S_bw_i=\lambda w_i<br>$$</p>
<p>&emsp;&emsp;到了这一步，根据矩阵特征向量的知识可以求出$S_w^{-1}S_b$的特征值，然后取前最大的k个特征值对应的特征向量（投影的超平面维度基向量）组成$W$矩阵，特征值越大所对应的特征向量分割性越好。</p>
<h2 id="4-LDA的一些细节注意点"><a href="#4-LDA的一些细节注意点" class="headerlink" title="4. LDA的一些细节注意点"></a>4. LDA的一些细节注意点</h2><ul>
<li>仔细分析$S_b$，这个矩阵的秩最大为$k-1$，个人理解是一旦知道(k-1)个$\mu_j-\mu$的值，最后一个必然可以根据定常量$\mu$求出来，因此和已知的(k-1)个行向量线性相关，因此特征向量最大为(k-1)，即投影向量的最大维度数。PCA不存在维度限制问题。</li>
<li>PCA投影的坐标系都是正交的，而LDA根据类别的标注关注分类能力，因此不保证投影到的坐标系是正交的（一般都不正交）。LDA假设数据服从单峰高斯分布，因此只有样本符合高斯分布才可以利用LDA分析（PCA也存在这样的问题），另外LDA可能会有过拟合风险。</li>
<li>LDA作为有监督学习降维，在寻找超平面过程中，是选择分类性能最好的方向，而LDA分类主要依赖的是均值而不是方差（PCA主要依赖的是方差），所以可以根据样本分类特性作出选择，当样本点依靠均值信息进行分类的时候，LDA比起PCA效果更优，当样本点依靠方差信息进行分类而不是均值信息时，LDA则不能进行有效分类因为LDA过度依赖均值信息，这个时候可以考虑PCA降维方法。</li>
<li>可以联系瑞利商和广义瑞利商的知识，加深对LDA和协方差矩阵的的理解。</li>
</ul>
<h2 id="5-参考文献"><a href="#5-参考文献" class="headerlink" title="5. 参考文献"></a>5. 参考文献</h2><ul>
<li><p>攻城狮凌风：线性判别分析LDA详解</p>
</li>
<li><p>刘建平：线性判别分析LDA原理总结</p>
</li>
</ul>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/fclefang/JavaScript/Geoserver/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">OSM搭建地图服务安装配置Geoserver</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>

  
</article>


<div class="share_jia">
	<!-- JiaThis Button BEGIN -->
	<div class="jiathis_style">
		<span class="jiathis_txt">Share to: &nbsp; </span>
		<a class="jiathis_button_facebook"></a> 
    <a class="jiathis_button_twitter"></a>
    <a class="jiathis_button_plus"></a> 
    <a class="jiathis_button_tsina"></a>
		<a class="jiathis_button_cqq"></a>
		<a class="jiathis_button_douban"></a>
		<a class="jiathis_button_weixin"></a>
		<a class="jiathis_button_tumblr"></a>
    <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	</div>
	<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1405949716054953" charset="utf-8"></script>
	<!-- JiaThis Button END -->
</div>






<div class="duoshuo">
	<!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="JavaScript/LDA" data-title="PCA and LDA" data-url="http://www.fclef.me/fclefang/JavaScript/LDA/"></div>
	<!-- 多说评论框 end -->
	<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"fclef"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<!-- 多说公共JS代码 end -->
</div>




</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2018 Fclefang
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>